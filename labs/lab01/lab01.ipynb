{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 80: Lab 01\n",
    "\n",
    "### Due Date: Tuesday January 14, Midnight (11:59 PM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Much like in DSC 10, this Jupyter Notebook contains the statements of the problems and provides code and markdown cells to display your answers to the problems. Unlike DSC 10, the notebook is *only* for displaying a readable version of your final answers. The coding work will be developed in an accompanying `lab01.py` file, that will be imported into the current notebook.\n",
    "\n",
    "Labs and programming assignments will be graded in (at most) two ways:\n",
    "1. The functions and classes in the accompanying python file will be tested (a la DSC 20),\n",
    "2. The notebook will be graded (for graphs and free response questions).\n",
    "\n",
    "**Do not change the function names in the `*.py` file**\n",
    "- The functions in the `*.py` file are how your assignment is graded, and they are graded by their name. The dictionary at the end of the file (`GRADED FUNCTIONS`) contains the \"grading list\". The final function in the file allows your doctests to check that all the necessary functions exist.\n",
    "- If you changed something you weren't supposed to, just use git to revert!\n",
    "\n",
    "**Tips for working in the Notebook**:\n",
    "- The notebooks serve to present you the questions and give you a place to present your results for later review.\n",
    "- The notebook on *lab assignments* are not graded (only the `.py` file).\n",
    "- Notebooks for PAs will serve as a final report for the assignment, and contain conclusions and answers to open ended questions that are graded.\n",
    "- The notebook serves as a nice environment for 'pre-development' and experimentation before designing your function in your `.py` file.\n",
    "\n",
    "**Tips for developing in the .py file**:\n",
    "- Do not change the function names in the starter code; grading is done using these function names.\n",
    "- Do not change the docstrings in the functions. These are there to tell you if your work is on the right track!\n",
    "- You are encouraged to write your own additional functions to solve the lab! \n",
    "    - Developing in python usually consists of larger files, with many short functions.\n",
    "    - You may write your other functions in an additional `.py` file that you import in `lab01.py` (much like we do in the notebook).\n",
    "- Always document your code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing code from `lab**.py`\n",
    "\n",
    "* We import our `.py` file that's contained in the same directory as this notebook.\n",
    "* We use the `autoreload` notebook extension to make changes to our `lab**.py` file immediately available in our notebook. Without this extension, we would need to restart the notebook kernel to see any changes to `lab**.py` in the notebook.\n",
    "    - `autoreload` is necessary because, upon import, `lab**.py` is compiled to bytecode (in the directory `__pycache__`). Subsequent imports of `lab**` merely import the existing compiled python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lab01 as lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Question 0 (EXAMPLE):**\n",
    "\n",
    "Write a function that takes in a possibly empty list of integers and:\n",
    "* Returns `True` if there exist two adjacent list elements that are consecutive integers.\n",
    "* Otherwise, returns `False`.\n",
    "\n",
    "For example, because `9` is next to `8`:\n",
    "```\n",
    ">>> lab.consecutive_ints([5,3,6,4,9,8])\n",
    "True\n",
    "```\n",
    "Whereas:\n",
    "```\n",
    ">>> lab.consecutive_ints([1,3,5,7,9])\n",
    "False\n",
    "```\n",
    "\n",
    "*Note*: This question is done for you, to demonstrate a completed homework problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop your code here (or in an IDE) if you'd like.\n",
    "# Though only code in lab01.py will be graded!\n",
    "def consecutive_ints(lst):\n",
    "    lst = []\n",
    "    n = len(lst)\n",
    "    min_num = min(lst)\n",
    "    max_num = max(lst)\n",
    "    if (max_num - min_num) + 1 == n:\n",
    "        visited = [False for i in range(n)]\n",
    "        for i in range(n):\n",
    "            if (visited[arr[i] - Min] != False): \n",
    "                return False\n",
    "            visited[arr[i] - Min] = True\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more cells if you'd like!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code in two ways:\n",
    "1. Run the cell below to test your code. You should also copy the cell and change the input to test further (i.e. write your own doctests)! Does it work for corner cases? Real-world data is **very messy** and you should expect your data processing code to break without thorough testing!\n",
    "2. Run doctests on `lab01.py` by running the following command on the commandline:\n",
    "```\n",
    "python -m doctest lab01.py\n",
    "```\n",
    "If the doctests pass, then there should be *no* output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your code!\n",
    "lab.consecutive_ints([1,3,2,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.consecutive_ints([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.consecutive_ints([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.consecutive_ints([5,3,6,4,9,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.consecutive_ints([1,3,5,7,9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Question 1 (median):**\n",
    "\n",
    "Write a function called *median* that takes a non-empty list of numbers, returning the median element of the list. If the list has even length, it should return the mean of the two elements in the middle. Do not use any imported libraries for this question; you may use any built-in function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median(nums):\n",
    "    middle = len(nums) // 2\n",
    "    nums.sort()\n",
    "    if not len(nums) % 2:\n",
    "        return (nums[middle - 1] + nums[middle]) / 2.0\n",
    "    return nums[middle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median([6, 5, 4, 3, 2]) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try this\n",
    "median([0, -1, 1, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median([50, 20, 15, 40]) == 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median([1, 2, 3, 4]) == 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Question 2 (List Distances):**\n",
    "\n",
    "Similar to Question 0, write a function that takes in a possibly empty list of integers and:\n",
    "* Returns `True` if there exist two list elements $i$ places apart, whose distance as integers is also $i$.\n",
    "* Otherwise, returns `False`.\n",
    "\n",
    "Assume your inputs tend to satisfy the condition, and the pair(s) saitifying the condition tend to be close together; design your function to run faster for this case. (Optimizing your code for an assumed distribution of incoming data is very common in data science).\n",
    "\n",
    "For example, because `3` and (the second) `5` are two places apart, and $|3-5| = 2$:\n",
    "```\n",
    ">>> lab.same_diff_ints([5,3,1,5,9,8])\n",
    "True\n",
    "```\n",
    "Whereas:\n",
    "```\n",
    ">>> lab.same_diff_ints([1,3,5,7,9])\n",
    "False\n",
    "```\n",
    "\n",
    "*Note*: Make sure to define some extreme test cases. Use the `%time` command to time your function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_diff_ints(ints):\n",
    "    for i in range(len(ints)):\n",
    "        for j in range(len(ints)):\n",
    "            if ints[i] != ints[j] and i - j == abs(ints[i] - ints[j]):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time lab.same_diff_ints([5,3,1,5,9,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_diff_ints([5,3,1,5,9,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_diff_ints([1,3,5,7,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,99999):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Strings and Files\n",
    "\n",
    "The following questions will help you (re)learn the basics of working with strings and reading data from files (which are read in as strings, by default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Question 3 (Prefixes):**\n",
    "\n",
    "Write a function `prefixes` that takes a string and returns a string of every consecutive prefix of the input string. For example, `prefixes('Data!')` should return `'DDaDatDataData!'`.  (See the doctests for more examples).\n",
    "\n",
    "Recall that [strings may be sliced](https://docs.python.org/3/tutorial/introduction.html#strings), like lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefixes(s):\n",
    "    result = ''\n",
    "    for i in range(1, len(s)+1):\n",
    "        result += s[:i]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes('Data!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes('Marina')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Question 4 (Evens reversed):**\n",
    "\n",
    "Write a function `evens_reversed` that takes in a non-negative integer $N$ and returns a string containing all even integers from $1$ to $N$ (inclusive) in reversed order, separated by spaces. Additionally, [zero pad](https://www.tutorialspoint.com/python/string_zfill.htm) each integer, so that each has the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evens_reversed(N):\n",
    "    result = ''\n",
    "    for i in range(N, 1, -1):\n",
    "        if i % 2 == 0:\n",
    "            result += str(i).zfill(len(str(N))) +' ' \n",
    "    return result[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evens_reversed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evens_reversed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "[Recall](https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files) that the built-in function `open` takes in a file path and returns *a file object* (sometimes called a *file handle*). Below are a few properties of file objects:\n",
    "\n",
    "* `open(path)` opens the file at location `path` for reading.\n",
    "* `open(path)` is an *iterable*, which contains successive lines of the file.\n",
    "* Once a file object is opened, after use it should be closed to avoid memory leaks. To ensure a file is closed once done, you should use a *context manager* as follows:\n",
    "```\n",
    "with open(path) as fh:\n",
    "    for line in fh:\n",
    "        process_line(line)\n",
    "```\n",
    "* To read the entire file into a string, use the read method:\n",
    "```\n",
    "with open(path) as fh:\n",
    "    s = fh.read()\n",
    "```\n",
    "However, you should be careful when reading an entire file into memory that the file isn't too big! *You should avoid this whenever possible!*\n",
    "\n",
    "**Question 5 (Reading Files):**\n",
    "\n",
    "Create a function `last_chars` that takes a file object and returns a string consisting of the last character of the line.\n",
    "\n",
    "*Remark:* A newline is the \"delimiter\" of the lines of a file, and doesn't count as part of the line (as the tests imply). Every other character is part of the line. For more info on this, see [the interpretation](https://en.wikipedia.org/wiki/Newline#Interpretation) of files as a 'newline delimited variables' file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_chars(fh):\n",
    "    file = fh.read()\n",
    "    result = ''\n",
    "    for i in range(len(file)):\n",
    "        if file[i] == '\\n':\n",
    "            result += file[i-1]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('data', 'chars.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_chars(open(fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `numpy` exercises\n",
    "\n",
    "For an introduction to arrays and `numpy` recall the relevant section of [DSC 10](https://www.inferentialthinking.com/chapters/05/1/Arrays.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6 (Basic Arrays):**\n",
    "\n",
    "Create the following functions using `numpy` methods satisfying the requirements given in each part. Your solutions should **not** contain any loops or list comprehensions.\n",
    "\n",
    "* A function `arr_1` that takes in a `numpy` array and adds to each element the square-root of the index of each element.\n",
    "\n",
    "* A function `arr_2` that takes in a `numpy` array of integers and returns a boolean array (i.e. an array of booleans) whose `ith` element is `True` if and only if the `ith` element of the input array is divisble by 16.\n",
    "\n",
    "* A function `arr_3` that takes in a `numpy` array of [stock prices](https://en.wikipedia.org/wiki/Stock) per share on successive days in USD and returns an array of growth rates. That is, the `ith` number of the output array should contain the rate of growth in stock price between the $i^{th}$ day to the $(i+1)^{th}$ day. The growth rate should be a proportion, rounded to the nearest hundredth.\n",
    "\n",
    "* Suppose:\n",
    "    - `A` is a `numpy` array of [stock prices](https://en.wikipedia.org/wiki/Stock) per share for a company on successive days in USD \n",
    "    - you start with \\\\$20, and put aside \\\\$20 at the end of each day to buy as much stock as possible the following day. \n",
    "    - Any money left-over after a given day is saved for possibly buying stock on a future day. \n",
    "    - Create a function `arr_4` that takes in `A` and returns the day on which you can buy at least one share from 'left-over' money. If this never happens, return `-1`. The first stock purchase occurs on day 0. *Note: you cannot buy fractions of a share of stock*.\n",
    "    \n",
    "*Example:* If the stock price is \\\\$3 every day, then the answer is 'day 1':\n",
    "* day 0: buy 6 shares; \\\\$2 left-over; \\\\$22 at end of day.\n",
    "* day 1: buy 7 shares; \\\\$1 left-over; \\\\$21 at end of day.\n",
    "This is more than the 6 shares that \\\\$20 can buy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('data', 'stocks.csv')\n",
    "stocks = np.array([float(x) for x in open(fp)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arr_1(A):\n",
    "    square = np.arange(len(A))**0.5\n",
    "    output = A + square\n",
    "\n",
    "    return output\n",
    "\n",
    "def arr_2(A):\n",
    "    result = (A%16) == 0\n",
    "    return result\n",
    "\n",
    "def arr_3(A):\n",
    "    return np.round(np.diff(A)/A[:-1],2)\n",
    "\n",
    "def arr_4(A):\n",
    "    day = np.cumsum(20%A)\n",
    "    result = (A > day).tolist().index(False)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([2, 4, 6, 7])\n",
    "out = arr_1(A)\n",
    "out\n",
    "isinstance(out, np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(out >= A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = arr_2(np.array([1, 2, 16, 17, 32, 33]))\n",
    "isinstance(out, np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.dtype == np.dtype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('data', 'stocks.csv')\n",
    "stocks = np.array([float(x) for x in open(fp)])\n",
    "out = arr_3(stocks)\n",
    "isinstance(out, np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.dtype == np.dtype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.max() == 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "stocks = np.array([3,3,3,3])\n",
    "out = arr_4(stocks)\n",
    "isinstance(out, numbers.Integral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Getting Started with Pandas\n",
    "\n",
    "The following questions will help you get comfortable with Pandas. These questions are similar to questions on tables in DSC 10; review the [textbook](https://www.inferentialthinking.com) as necessary. As always for Pandas questions:\n",
    "1. Avoid writing loops through the rows of the dataset to do the problem, and\n",
    "2. Test the output/correctness of your code with the help of the dataset given, but be sure your code will also run on data \"like\" the dataset given (sampling rows using the `.sample` method is useful for this!).\n",
    "\n",
    "**Question 7 (Pandas basics):**\n",
    "\n",
    "Read in the file `movies_by_year.csv` in the `data` directory and understand the dataset by answering the following questions. To do this, create a function `movie_stats` that takes in a dataframe like `movies` and returns a series containing the following statistics:\n",
    "* The number of years covered by the dataset (`num_years`).\n",
    "* The total number of movies made over all years in the dataset (`tot_movies`).\n",
    "* The year with the fewest number of movies made; a tie should return the earliest year (`yr_fewest_movies`).\n",
    "* The average amount of money grossed over all the years in the dataset (`avg_gross`).\n",
    "* The year with the highest gross *per movie* (`highest_per_movie`).\n",
    "* The name of the top movie during the second-lowest (total) grossing year (`second_lowest`).\n",
    "* The average number of movies made the year *after* a Harry Potter movie was the #1 movie (`avg_after_harry`).\n",
    "\n",
    "The index of the output series are given in parenthesis above.\n",
    "\n",
    "*Note*: Your function should work on a dataset of the same format that contains information from other years. You may assume that none of the answers involving ranking returns a tie.\n",
    "\n",
    "*Note*: To make sure your function still runs, in the event that one of the 7 parts throws an exception (e.g. due to a very incorrect answer), use `Try... Except...` structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Total Gross</th>\n",
       "      <th>Number of Movies</th>\n",
       "      <th>#1 Movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>11128.5</td>\n",
       "      <td>702</td>\n",
       "      <td>Star Wars: The Force Awakens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014</td>\n",
       "      <td>10360.8</td>\n",
       "      <td>702</td>\n",
       "      <td>American Sniper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>10923.6</td>\n",
       "      <td>688</td>\n",
       "      <td>Catching Fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012</td>\n",
       "      <td>10837.4</td>\n",
       "      <td>667</td>\n",
       "      <td>The Avengers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011</td>\n",
       "      <td>10174.3</td>\n",
       "      <td>602</td>\n",
       "      <td>Harry Potter / Deathly Hallows (P2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2010</td>\n",
       "      <td>10565.6</td>\n",
       "      <td>536</td>\n",
       "      <td>Toy Story 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2009</td>\n",
       "      <td>10595.5</td>\n",
       "      <td>521</td>\n",
       "      <td>Avatar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2008</td>\n",
       "      <td>9630.7</td>\n",
       "      <td>608</td>\n",
       "      <td>The Dark Knight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2007</td>\n",
       "      <td>9663.8</td>\n",
       "      <td>631</td>\n",
       "      <td>Spider-Man 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2006</td>\n",
       "      <td>9209.5</td>\n",
       "      <td>608</td>\n",
       "      <td>Dead Man's Chest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2005</td>\n",
       "      <td>8840.5</td>\n",
       "      <td>547</td>\n",
       "      <td>Revenge of the Sith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2004</td>\n",
       "      <td>9380.5</td>\n",
       "      <td>551</td>\n",
       "      <td>Shrek 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2003</td>\n",
       "      <td>9239.7</td>\n",
       "      <td>506</td>\n",
       "      <td>Return of the King</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2002</td>\n",
       "      <td>9155.0</td>\n",
       "      <td>479</td>\n",
       "      <td>Spider-Man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2001</td>\n",
       "      <td>8412.5</td>\n",
       "      <td>482</td>\n",
       "      <td>Harry Potter / Sorcerer's Stone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2000</td>\n",
       "      <td>7661.0</td>\n",
       "      <td>478</td>\n",
       "      <td>The Grinch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1999</td>\n",
       "      <td>7448.0</td>\n",
       "      <td>461</td>\n",
       "      <td>The Phantom Menace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1998</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>509</td>\n",
       "      <td>Saving Private Ryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1997</td>\n",
       "      <td>6365.9</td>\n",
       "      <td>510</td>\n",
       "      <td>Titanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1996</td>\n",
       "      <td>5911.5</td>\n",
       "      <td>471</td>\n",
       "      <td>Independence Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1995</td>\n",
       "      <td>5493.5</td>\n",
       "      <td>411</td>\n",
       "      <td>Toy Story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1994</td>\n",
       "      <td>5396.2</td>\n",
       "      <td>453</td>\n",
       "      <td>Forrest Gump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1993</td>\n",
       "      <td>5154.2</td>\n",
       "      <td>462</td>\n",
       "      <td>Jurassic Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1992</td>\n",
       "      <td>4871.0</td>\n",
       "      <td>480</td>\n",
       "      <td>Aladdin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1991</td>\n",
       "      <td>4803.2</td>\n",
       "      <td>458</td>\n",
       "      <td>Terminator 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1990</td>\n",
       "      <td>5021.8</td>\n",
       "      <td>410</td>\n",
       "      <td>Home Alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1989</td>\n",
       "      <td>5033.4</td>\n",
       "      <td>502</td>\n",
       "      <td>Batman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1988</td>\n",
       "      <td>4458.4</td>\n",
       "      <td>510</td>\n",
       "      <td>Rain Man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1987</td>\n",
       "      <td>4252.9</td>\n",
       "      <td>509</td>\n",
       "      <td>Three Men and a Baby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1986</td>\n",
       "      <td>3778.0</td>\n",
       "      <td>451</td>\n",
       "      <td>Top Gun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1985</td>\n",
       "      <td>3749.2</td>\n",
       "      <td>470</td>\n",
       "      <td>Back to the Future</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1984</td>\n",
       "      <td>4031.0</td>\n",
       "      <td>536</td>\n",
       "      <td>Beverly Hills Cop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1983</td>\n",
       "      <td>3766.0</td>\n",
       "      <td>495</td>\n",
       "      <td>Return of the Jedi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1982</td>\n",
       "      <td>3453.0</td>\n",
       "      <td>428</td>\n",
       "      <td>E.T.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year  Total Gross  Number of Movies                             #1 Movie\n",
       "0   2015      11128.5               702         Star Wars: The Force Awakens\n",
       "1   2014      10360.8               702                      American Sniper\n",
       "2   2013      10923.6               688                        Catching Fire\n",
       "3   2012      10837.4               667                         The Avengers\n",
       "4   2011      10174.3               602  Harry Potter / Deathly Hallows (P2)\n",
       "5   2010      10565.6               536                          Toy Story 3\n",
       "6   2009      10595.5               521                               Avatar\n",
       "7   2008       9630.7               608                      The Dark Knight\n",
       "8   2007       9663.8               631                         Spider-Man 3\n",
       "9   2006       9209.5               608                     Dead Man's Chest\n",
       "10  2005       8840.5               547                  Revenge of the Sith\n",
       "11  2004       9380.5               551                              Shrek 2\n",
       "12  2003       9239.7               506                   Return of the King\n",
       "13  2002       9155.0               479                           Spider-Man\n",
       "14  2001       8412.5               482      Harry Potter / Sorcerer's Stone\n",
       "15  2000       7661.0               478                           The Grinch\n",
       "16  1999       7448.0               461                   The Phantom Menace\n",
       "17  1998       6949.0               509                  Saving Private Ryan\n",
       "18  1997       6365.9               510                              Titanic\n",
       "19  1996       5911.5               471                     Independence Day\n",
       "20  1995       5493.5               411                            Toy Story\n",
       "21  1994       5396.2               453                         Forrest Gump\n",
       "22  1993       5154.2               462                        Jurassic Park\n",
       "23  1992       4871.0               480                              Aladdin\n",
       "24  1991       4803.2               458                         Terminator 2\n",
       "25  1990       5021.8               410                           Home Alone\n",
       "26  1989       5033.4               502                               Batman\n",
       "27  1988       4458.4               510                             Rain Man\n",
       "28  1987       4252.9               509                 Three Men and a Baby\n",
       "29  1986       3778.0               451                              Top Gun\n",
       "30  1985       3749.2               470                   Back to the Future\n",
       "31  1984       4031.0               536                    Beverly Hills Cop\n",
       "32  1983       3766.0               495                   Return of the Jedi\n",
       "33  1982       3453.0               428                                 E.T."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_fp = os.path.join('data', 'movies_by_year.csv')\n",
    "movies = pd.read_csv(movie_fp)\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_stats(movies):\n",
    "    #number of years\n",
    "    try:\n",
    "        num_years = max(movies['Year']) - min(movies['Year'])\n",
    "    except:\n",
    "        num_years = None\n",
    "    #total movies\n",
    "    try:\n",
    "        tot_movies = sum(movies['Number of Movies'])\n",
    "    except:\n",
    "        tot_movies = None\n",
    "    #Year of fewest movies\n",
    "    try:\n",
    "        sorted_byNOM = movies[movies['Number of Movies'] == min(movies['Number of Movies'])]\n",
    "        yr_fewest_movies = sorted_byNOM.sort_values(by = 'Year')['Year'].values[0]\n",
    "    except:\n",
    "        yr_fewest_movies = None\n",
    "    #Average gross\n",
    "    try: \n",
    "        avg_gross = np.mean(movies['Total Gross'].tolist())\n",
    "    except:\n",
    "        avg_gross = None\n",
    "    #Highest gross per movie   \n",
    "    try:\n",
    "        gross = movies['Total Gross'].values\n",
    "        num =  movies['Number of Movies'].values\n",
    "        sub = movies\n",
    "        sub['gross per movie'] = gross/num\n",
    "        highest_per_movie = sub.sort_values(by = 'gross per movie',ascending = False)['Year'].values[0]\n",
    "    except:\n",
    "        highest_per_movie = None\n",
    "    #second lowest movie\n",
    "    try:\n",
    "        second_lowest= movies.sort_values(by='Total Gross')['#1 Movie'].values[1]\n",
    "    except:\n",
    "        second_lowest = None\n",
    "    #averge after Harry Potter\n",
    "    try:\n",
    "        find_HP = lambda i : True if 'Harry Potter' in i else False\n",
    "        Harry = list(map(find_HP,movies['#1 Movie'].values))\n",
    "        copy = movies\n",
    "        copy['Harry'] = Harry\n",
    "        sub = copy[copy['Harry'] == True]\n",
    "        second = sub['Year'].values+1\n",
    "        A = movies[movies['Year'].isin(second) ]\n",
    "        avg_after_harry = np.mean(A['Number of Movies'].values)\n",
    "    except:\n",
    "        avg_after_harry = None\n",
    "    \n",
    "    contents = [num_years, tot_movies,yr_fewest_movies,avg_gross,highest_per_movie,second_lowest,avg_after_harry]\n",
    "    labels = ['num_years', 'tot_movies', 'yr_fewest_movies', 'avg_gross', 'highest_per_movie','second_lowest','avg_after_harry']\n",
    "    result = pd.Series(contents, index = labels)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_stats(movies):\n",
    "    \"\"\"\n",
    "    movies_stats returns a series as specified in the notebook.\n",
    "    :param movies: a dataframe of summaries of\n",
    "    movies per year as found in `movies_by_year.csv`\n",
    "    :return: a series with index specified in the notebook.\n",
    "    :Example:\n",
    "    >>> movie_fp = os.path.join('data', 'movies_by_year.csv')\n",
    "    >>> movies = pd.read_csv(movie_fp)\n",
    "    >>> out = movie_stats(movies)\n",
    "    >>> isinstance(out, pd.Series)\n",
    "    True\n",
    "    >>> 'num_years' in out.index\n",
    "    True\n",
    "    >>> isinstance(out.loc['second_lowest'], str)\n",
    "    True\n",
    "    \"\"\"\n",
    "    # num_years\n",
    "    num_years = max(movies['Year']) - min(movies['Year'])\n",
    "\n",
    "    # tot_movies\n",
    "    tot_movies = sum(movies['Number of Movies'])\n",
    "\n",
    "    # yr_fewest_movies\n",
    "    fewest_movs = min(movies['Number of Movies'])\n",
    "    bool_subset = movies['Number of Movies'] == fewest_movs\n",
    "    yr_fewest_movies = min(movies.loc[bool_subset]['Year'])\n",
    "\n",
    "    # avg_gross\n",
    "    gross_col = movies['Total Gross']\n",
    "    avg_gross = (sum(gross_col) / len(gross_col))\n",
    "\n",
    "    # highest_per_movie\n",
    "    # gpm = gross per movie\n",
    "    gpm_frame = movies.join(pd.DataFrame({'gpm':\n",
    "                                         movies['Total Gross'] /\n",
    "                                         movies['Number of Movies']}))\n",
    "    max_gpm = max(gpm_frame['gpm'])\n",
    "    highest_per_movie = gpm_frame.loc[gpm_frame['gpm'] == max_gpm]['Year']\n",
    "    highest_per_movie = highest_per_movie.reset_index(drop=True)[0]\n",
    "\n",
    "    # second_lowest\n",
    "    sorted_grosses = movies.sort_values('Total Gross', ascending = True)\n",
    "    sorted_indices = sorted_grosses.reset_index(drop=True)\n",
    "    second_lowest = sorted_indices['#1 Movie'][1]\n",
    "\n",
    "    # avg_after_harry\n",
    "    hpot = movies['#1 Movie'].str.contains('Harry Potter')\n",
    "    hpot_indx = np.array(movies.index[hpot].tolist())\n",
    "    movies_after_hpot = movies.loc[hpot_indx - 1]['Number of Movies'].mean()\n",
    "    avg_after_harry = movies_after_hpot\n",
    "\n",
    "    labels = ['num_years', 'tot_movies', 'yr_fewest_movies',\n",
    "              'avg_gross', 'highest_per_movie', 'second_lowest',\n",
    "              'avg_after_harry']\n",
    "    movie_stats = pd.Series([num_years,\n",
    "                            tot_movies,\n",
    "                            yr_fewest_movies,\n",
    "                            avg_gross,\n",
    "                            highest_per_movie,\n",
    "                            second_lowest,\n",
    "                            avg_after_harry],\n",
    "                            labels)\n",
    "\n",
    "    return movies_after_hpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Total Gross</th>\n",
       "      <th>Number of Movies</th>\n",
       "      <th>#1 Movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>11128.5</td>\n",
       "      <td>702</td>\n",
       "      <td>Star Wars: The Force Awakens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014</td>\n",
       "      <td>10360.8</td>\n",
       "      <td>702</td>\n",
       "      <td>American Sniper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>10923.6</td>\n",
       "      <td>688</td>\n",
       "      <td>Catching Fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012</td>\n",
       "      <td>10837.4</td>\n",
       "      <td>667</td>\n",
       "      <td>The Avengers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011</td>\n",
       "      <td>10174.3</td>\n",
       "      <td>602</td>\n",
       "      <td>Harry Potter / Deathly Hallows (P2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2010</td>\n",
       "      <td>10565.6</td>\n",
       "      <td>536</td>\n",
       "      <td>Toy Story 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2009</td>\n",
       "      <td>10595.5</td>\n",
       "      <td>521</td>\n",
       "      <td>Avatar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2008</td>\n",
       "      <td>9630.7</td>\n",
       "      <td>608</td>\n",
       "      <td>The Dark Knight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2007</td>\n",
       "      <td>9663.8</td>\n",
       "      <td>631</td>\n",
       "      <td>Spider-Man 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2006</td>\n",
       "      <td>9209.5</td>\n",
       "      <td>608</td>\n",
       "      <td>Dead Man's Chest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2005</td>\n",
       "      <td>8840.5</td>\n",
       "      <td>547</td>\n",
       "      <td>Revenge of the Sith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2004</td>\n",
       "      <td>9380.5</td>\n",
       "      <td>551</td>\n",
       "      <td>Shrek 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2003</td>\n",
       "      <td>9239.7</td>\n",
       "      <td>506</td>\n",
       "      <td>Return of the King</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2002</td>\n",
       "      <td>9155.0</td>\n",
       "      <td>479</td>\n",
       "      <td>Spider-Man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2001</td>\n",
       "      <td>8412.5</td>\n",
       "      <td>482</td>\n",
       "      <td>Harry Potter / Sorcerer's Stone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2000</td>\n",
       "      <td>7661.0</td>\n",
       "      <td>478</td>\n",
       "      <td>The Grinch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1999</td>\n",
       "      <td>7448.0</td>\n",
       "      <td>461</td>\n",
       "      <td>The Phantom Menace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1998</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>509</td>\n",
       "      <td>Saving Private Ryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1997</td>\n",
       "      <td>6365.9</td>\n",
       "      <td>510</td>\n",
       "      <td>Titanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1996</td>\n",
       "      <td>5911.5</td>\n",
       "      <td>471</td>\n",
       "      <td>Independence Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1995</td>\n",
       "      <td>5493.5</td>\n",
       "      <td>411</td>\n",
       "      <td>Toy Story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1994</td>\n",
       "      <td>5396.2</td>\n",
       "      <td>453</td>\n",
       "      <td>Forrest Gump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1993</td>\n",
       "      <td>5154.2</td>\n",
       "      <td>462</td>\n",
       "      <td>Jurassic Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1992</td>\n",
       "      <td>4871.0</td>\n",
       "      <td>480</td>\n",
       "      <td>Aladdin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1991</td>\n",
       "      <td>4803.2</td>\n",
       "      <td>458</td>\n",
       "      <td>Terminator 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1990</td>\n",
       "      <td>5021.8</td>\n",
       "      <td>410</td>\n",
       "      <td>Home Alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1989</td>\n",
       "      <td>5033.4</td>\n",
       "      <td>502</td>\n",
       "      <td>Batman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1988</td>\n",
       "      <td>4458.4</td>\n",
       "      <td>510</td>\n",
       "      <td>Rain Man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1987</td>\n",
       "      <td>4252.9</td>\n",
       "      <td>509</td>\n",
       "      <td>Three Men and a Baby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1986</td>\n",
       "      <td>3778.0</td>\n",
       "      <td>451</td>\n",
       "      <td>Top Gun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1985</td>\n",
       "      <td>3749.2</td>\n",
       "      <td>470</td>\n",
       "      <td>Back to the Future</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1984</td>\n",
       "      <td>4031.0</td>\n",
       "      <td>536</td>\n",
       "      <td>Beverly Hills Cop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1983</td>\n",
       "      <td>3766.0</td>\n",
       "      <td>495</td>\n",
       "      <td>Return of the Jedi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1982</td>\n",
       "      <td>3453.0</td>\n",
       "      <td>428</td>\n",
       "      <td>E.T.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year  Total Gross  Number of Movies                             #1 Movie\n",
       "0   2015      11128.5               702         Star Wars: The Force Awakens\n",
       "1   2014      10360.8               702                      American Sniper\n",
       "2   2013      10923.6               688                        Catching Fire\n",
       "3   2012      10837.4               667                         The Avengers\n",
       "4   2011      10174.3               602  Harry Potter / Deathly Hallows (P2)\n",
       "5   2010      10565.6               536                          Toy Story 3\n",
       "6   2009      10595.5               521                               Avatar\n",
       "7   2008       9630.7               608                      The Dark Knight\n",
       "8   2007       9663.8               631                         Spider-Man 3\n",
       "9   2006       9209.5               608                     Dead Man's Chest\n",
       "10  2005       8840.5               547                  Revenge of the Sith\n",
       "11  2004       9380.5               551                              Shrek 2\n",
       "12  2003       9239.7               506                   Return of the King\n",
       "13  2002       9155.0               479                           Spider-Man\n",
       "14  2001       8412.5               482      Harry Potter / Sorcerer's Stone\n",
       "15  2000       7661.0               478                           The Grinch\n",
       "16  1999       7448.0               461                   The Phantom Menace\n",
       "17  1998       6949.0               509                  Saving Private Ryan\n",
       "18  1997       6365.9               510                              Titanic\n",
       "19  1996       5911.5               471                     Independence Day\n",
       "20  1995       5493.5               411                            Toy Story\n",
       "21  1994       5396.2               453                         Forrest Gump\n",
       "22  1993       5154.2               462                        Jurassic Park\n",
       "23  1992       4871.0               480                              Aladdin\n",
       "24  1991       4803.2               458                         Terminator 2\n",
       "25  1990       5021.8               410                           Home Alone\n",
       "26  1989       5033.4               502                               Batman\n",
       "27  1988       4458.4               510                             Rain Man\n",
       "28  1987       4252.9               509                 Three Men and a Baby\n",
       "29  1986       3778.0               451                              Top Gun\n",
       "30  1985       3749.2               470                   Back to the Future\n",
       "31  1984       4031.0               536                    Beverly Hills Cop\n",
       "32  1983       3766.0               495                   Return of the Jedi\n",
       "33  1982       3453.0               428                                 E.T."
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_fp = os.path.join('data', 'movies_by_year.csv')\n",
    "movies = pd.read_csv(movie_fp)\n",
    "out = movie_stats(movies)\n",
    "isinstance(out, pd.Series)\n",
    "out\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'num_years' in out.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(out.loc['second_lowest'], str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## CSV Files\n",
    "\n",
    "**Question 8 (Reading malformed csv files):**\n",
    "\n",
    "`malformed.csv` contains a file of comma-separated values, containing the following fields:\n",
    "\n",
    "\n",
    "|column name|description|type|\n",
    "|---|---|---|\n",
    "|first|first name of person|str|\n",
    "|last|last name of person|str|\n",
    "|weight|weight of person (lbs)|float|\n",
    "|height|height of person (in)|float|\n",
    "|geo|location of person; comma-separated latitude/longitude|str|\n",
    "\n",
    "Unfortunately, the entries contains errors that cause the Pandas `read_csv` function to fail parsing the file with the default settings. Instead, you must read in the file manually using Python's built-in `open` function.\n",
    "\n",
    "Clean the csv file into a Pandas DataFrame with columns as described in the table above, by creating a function called `parse_malformed` that takes in a file path and returns a parsed, properly-typed dataframe. The dataframe should contain columns as described in the table above (with the specified types); it should agree with `pd.read_csv` when the lines are not malformed.\n",
    "\n",
    "\n",
    "*Note:* Assume that the given csv file is a sample of a larger file; you will be graded against a **different** sample of the larger file that has the same type of parsing errors. That is, you should **not** hard-code your cleaning of the data to specific errors on specific lines in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_malformed(fp):\n",
    "    with open(fp) as fh:\n",
    "        file = fh.read()\n",
    "    modified_file = [i.split(',') for i in file.split()]\n",
    "    for i in modified_file:\n",
    "        if '' in i:\n",
    "            i.remove('')\n",
    "    first = [i[0] for i in modified_file[1:]]\n",
    "    last = [i[1] for i in modified_file[1:]]\n",
    "\n",
    "    weight = [float(i[2].strip('\"')) for i in modified_file[1:]]\n",
    "    height = [float(i[3].strip('\"')) for i in modified_file[1:]]\n",
    "    geo = [(i[4]+','+i[5]).strip('\"') for i in modified_file[1:]]\n",
    "    new_frame = pd.DataFrame({modified_file[0][0]:first,modified_file[0][1]:last,modified_file[0][2]:weight,\n",
    "                       modified_file[0][3]:height,modified_file[0][4]:geo})\n",
    "\n",
    "    return new_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_malformed(fp):\n",
    "    \"\"\"\n",
    "    Parses and loads the malformed csv data into a \n",
    "    properly formatted dataframe (as described in \n",
    "    the question).\n",
    "    :param fh: file handle for the malformed csv-file.\n",
    "    :returns: a Pandas DataFrame of the data, \n",
    "    as specificed in the question statement.\n",
    "    :Example:\n",
    "    >>> fp = os.path.join('data', 'malformed.csv')\n",
    "    >>> df = parse_malformed(fp)\n",
    "    >>> cols = ['first', 'last', 'weight', 'height', 'geo']\n",
    "    >>> list(df.columns) == cols\n",
    "    True\n",
    "    >>> df['last'].dtype == np.dtype('O')\n",
    "    True\n",
    "    >>> df['height'].dtype == np.dtype('float64')\n",
    "    True\n",
    "    >>> df['geo'].str.contains(',').all()\n",
    "    True\n",
    "    >>> len(df) == 100\n",
    "    True\n",
    "    >>> dg = pd.read_csv(fp, nrows=4, skiprows=10, names=cols)\n",
    "    >>> dg.index = range(9, 13)\n",
    "    >>> (dg == df.iloc[9:13]).all().all()\n",
    "    True\n",
    "    \"\"\"\n",
    "    regexp_fn = r'(\\w+)\\W.*\\n'\n",
    "    data_type_fn = [('first', np.str_, 32)]\n",
    "    first_names = np.fromregex(fp, regexp_fn, dtype=data_type_fn)[1:]\n",
    "\n",
    "    regexp_ln = r'\\W+(\\w+)\\W.*\\n'\n",
    "    data_type_ln = [('last', np.str_, 32)]\n",
    "    last_names = np.fromregex(fp, regexp_ln, dtype=data_type_ln)[1:]\n",
    "\n",
    "    regexp_w = r'\\w+\\W+(\\d+\\.\\d+)\\W+.*\\n'\n",
    "    data_type_w = [('weight', np.float64)]\n",
    "    weights = np.fromregex(fp, regexp_w, dtype=data_type_w)\n",
    "\n",
    "    regexp_h = r'\\d\\W+(\\d+\\.\\d+)\\W+\\d+.*\\n'\n",
    "    data_type_h = [('height', np.float64)]\n",
    "    heights = np.fromregex(fp, regexp_h, dtype=data_type_h)\n",
    "\n",
    "    regexp_g = r'\\d\\W+\\d+\\.\\d+\\W+(\\W?\\d+.\\d+\\W+\\d+.\\d+).*\\n'\n",
    "    data_type_g = [('geo', np.str_, 32)]\n",
    "    geo_locs = np.fromregex(fp, regexp_g, dtype=data_type_g)\n",
    "\n",
    "\n",
    "    full_frame = pd.DataFrame(first_names)\n",
    "    full_frame = full_frame.join([pd.DataFrame(last_names),\n",
    "                                  pd.DataFrame(weights),\n",
    "                                  pd.DataFrame(heights),\n",
    "                                  pd.DataFrame(geo_locs)])\n",
    "\n",
    "    return full_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "      <th>geo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Julia</td>\n",
       "      <td>Wagner</td>\n",
       "      <td>142.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>39.8,15.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Angelica</td>\n",
       "      <td>Rija</td>\n",
       "      <td>155.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>38.2,-71.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tyler</td>\n",
       "      <td>Micajah</td>\n",
       "      <td>116.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>38.0,6.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kathleen</td>\n",
       "      <td>Nakea</td>\n",
       "      <td>163.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>36.3,-86.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Axel</td>\n",
       "      <td>Ronit</td>\n",
       "      <td>95.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>36.8,128.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Amiya</td>\n",
       "      <td>Kyona</td>\n",
       "      <td>130.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>36.3,114.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Torrey</td>\n",
       "      <td>Joshuacaleb</td>\n",
       "      <td>105.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>38.3,145.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mariah</td>\n",
       "      <td>Alese</td>\n",
       "      <td>149.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>36.1,45.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Grayson</td>\n",
       "      <td>Daimen</td>\n",
       "      <td>140.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>38.1,-72.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Yvette</td>\n",
       "      <td>Trayce</td>\n",
       "      <td>179.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>36.9,-8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Cody</td>\n",
       "      <td>Hatim</td>\n",
       "      <td>150.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>38.0,-7.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Marissa</td>\n",
       "      <td>Daud</td>\n",
       "      <td>135.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>37.3,11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Logan</td>\n",
       "      <td>Cristel</td>\n",
       "      <td>133.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>35.5,-110.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Kaiyah</td>\n",
       "      <td>Brinden</td>\n",
       "      <td>187.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>34.8,83.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ivan</td>\n",
       "      <td>Devyne</td>\n",
       "      <td>193.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>36.6,262.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Shamaria</td>\n",
       "      <td>Aldrick\"</td>\n",
       "      <td>139.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>38.5,-94.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Travis</td>\n",
       "      <td>Anavictoria</td>\n",
       "      <td>117.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>36.3,69.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>Dalynn</td>\n",
       "      <td>171.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>37.3,-27.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Alina</td>\n",
       "      <td>Danniell</td>\n",
       "      <td>105.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>37.4,314.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Cameron</td>\n",
       "      <td>Angelica</td>\n",
       "      <td>139.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>38.8,-79.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Madison</td>\n",
       "      <td>Barkley</td>\n",
       "      <td>120.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>38.2,86.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Jackson</td>\n",
       "      <td>Taylr</td>\n",
       "      <td>113.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>36.7,56.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Agustin</td>\n",
       "      <td>Stephanye</td>\n",
       "      <td>91.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>36.4,54.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Janesha</td>\n",
       "      <td>Jhayla</td>\n",
       "      <td>143.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>35.9,-70.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Nickolas</td>\n",
       "      <td>Karenna</td>\n",
       "      <td>159.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>35.9,-73.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Stacy</td>\n",
       "      <td>Meaghen</td>\n",
       "      <td>149.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>36.6,-27.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Matthew</td>\n",
       "      <td>Kalis</td>\n",
       "      <td>166.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>37.8,0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Rayona</td>\n",
       "      <td>Treniece</td>\n",
       "      <td>151.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>37.2,140.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Jack</td>\n",
       "      <td>Hanai</td>\n",
       "      <td>164.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>38.6,-63.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Hayley</td>\n",
       "      <td>Zsazsa</td>\n",
       "      <td>146.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>36.2,-2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Shay</td>\n",
       "      <td>Marcea</td>\n",
       "      <td>103.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>36.0,-33.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Adrianna</td>\n",
       "      <td>Karsten</td>\n",
       "      <td>157.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>34.8,36.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Max</td>\n",
       "      <td>Nairi</td>\n",
       "      <td>113.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>36.9,12.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Suzanne</td>\n",
       "      <td>Dearion</td>\n",
       "      <td>129.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>36.9,-70.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Camryn</td>\n",
       "      <td>Tasnim</td>\n",
       "      <td>126.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>36.5,19.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Cielo</td>\n",
       "      <td>Saylah</td>\n",
       "      <td>103.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>36.1,83.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Annastasia</td>\n",
       "      <td>Averi</td>\n",
       "      <td>91.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>37.1,-43.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Hannah</td>\n",
       "      <td>Jerit</td>\n",
       "      <td>142.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>36.6,10.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Jovita</td>\n",
       "      <td>Marquaveon</td>\n",
       "      <td>150.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>38.2,-32.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Jasmine</td>\n",
       "      <td>Jahmal\"</td>\n",
       "      <td>107.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>38.5,9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Cheyenne</td>\n",
       "      <td>Gerrid</td>\n",
       "      <td>55.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>36.6,68.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Karma\"</td>\n",
       "      <td>Nameer</td>\n",
       "      <td>131.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>37.5,69.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Madeline</td>\n",
       "      <td>Cira</td>\n",
       "      <td>128.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>37.4,149.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Connor</td>\n",
       "      <td>Sheridyn</td>\n",
       "      <td>141.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>36.8,-80.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Sarah</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>180.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>36.7,-5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Victoria</td>\n",
       "      <td>Maliik</td>\n",
       "      <td>164.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>36.3,151.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>George</td>\n",
       "      <td>Tyshonna</td>\n",
       "      <td>105.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>37.5,-91.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Marquis</td>\n",
       "      <td>Kylar</td>\n",
       "      <td>137.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>38.2,228.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Marlene</td>\n",
       "      <td>Fatuma</td>\n",
       "      <td>117.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>36.4,-9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Kathryn</td>\n",
       "      <td>Diondra</td>\n",
       "      <td>149.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>38.3,68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Jonathan</td>\n",
       "      <td>Jersey</td>\n",
       "      <td>107.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>36.0,11.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Joshua</td>\n",
       "      <td>Caileen</td>\n",
       "      <td>181.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>37.3,79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Emily</td>\n",
       "      <td>Leonid</td>\n",
       "      <td>146.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>37.8,-68.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Jacqualyn</td>\n",
       "      <td>Angelea</td>\n",
       "      <td>116.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>36.4,-31.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Gionna</td>\n",
       "      <td>Pamala</td>\n",
       "      <td>180.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>38.6,-28.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Yasmeen</td>\n",
       "      <td>Jahron</td>\n",
       "      <td>135.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>38.3,-127.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Meghan</td>\n",
       "      <td>Carlyann</td>\n",
       "      <td>101.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>36.6,80.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Tess</td>\n",
       "      <td>Shree</td>\n",
       "      <td>146.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>38.8,64.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Maria</td>\n",
       "      <td>Kalvyn</td>\n",
       "      <td>115.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>37.1,-90.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Lexie</td>\n",
       "      <td>Cheyenna</td>\n",
       "      <td>151.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>35.9,86.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         first         last  weight  height          geo\n",
       "0        Julia       Wagner   142.0    86.0    39.8,15.4\n",
       "1     Angelica         Rija   155.0    56.0   38.2,-71.7\n",
       "2        Tyler      Micajah   116.0    73.0     38.0,6.9\n",
       "3     Kathleen        Nakea   163.0    69.0   36.3,-86.8\n",
       "4         Axel        Ronit    95.0    74.0   36.8,128.2\n",
       "5        Amiya        Kyona   130.0    72.0   36.3,114.5\n",
       "6       Torrey  Joshuacaleb   105.0    79.0   38.3,145.1\n",
       "7       Mariah        Alese   149.0    68.0    36.1,45.7\n",
       "8      Grayson       Daimen   140.0    80.0   38.1,-72.6\n",
       "9       Yvette       Trayce   179.0    67.0    36.9,-8.3\n",
       "10        Cody        Hatim   150.0    63.0    38.0,-7.3\n",
       "11     Marissa         Daud   135.0    58.0    37.3,11.0\n",
       "12       Logan      Cristel   133.0    67.0  35.5,-110.2\n",
       "13      Kaiyah      Brinden   187.0    82.0    34.8,83.2\n",
       "14        Ivan       Devyne   193.0    54.0   36.6,262.0\n",
       "15    Shamaria     Aldrick\"   139.0    73.0   38.5,-94.6\n",
       "16      Travis  Anavictoria   117.0    62.0    36.3,69.5\n",
       "17     Kennedy       Dalynn   171.0    77.0   37.3,-27.5\n",
       "18       Alina     Danniell   105.0    55.0   37.4,314.7\n",
       "19     Cameron     Angelica   139.0    56.0   38.8,-79.3\n",
       "20     Madison      Barkley   120.0    69.0    38.2,86.1\n",
       "21     Jackson        Taylr   113.0    78.0    36.7,56.7\n",
       "22     Agustin    Stephanye    91.0    62.0    36.4,54.5\n",
       "23     Janesha       Jhayla   143.0    64.0   35.9,-70.5\n",
       "24    Nickolas      Karenna   159.0    75.0   35.9,-73.9\n",
       "25       Stacy      Meaghen   149.0    68.0   36.6,-27.7\n",
       "26     Matthew        Kalis   166.0    66.0     37.8,0.2\n",
       "27      Rayona     Treniece   151.0    59.0   37.2,140.1\n",
       "28        Jack        Hanai   164.0    70.0   38.6,-63.6\n",
       "29      Hayley       Zsazsa   146.0    62.0    36.2,-2.6\n",
       "..         ...          ...     ...     ...          ...\n",
       "70        Shay       Marcea   103.0    65.0   36.0,-33.1\n",
       "71    Adrianna      Karsten   157.0    81.0    34.8,36.1\n",
       "72         Max        Nairi   113.0    67.0    36.9,12.2\n",
       "73     Suzanne      Dearion   129.0    70.0   36.9,-70.4\n",
       "74      Camryn       Tasnim   126.0    63.0    36.5,19.8\n",
       "75       Cielo       Saylah   103.0    65.0    36.1,83.1\n",
       "76  Annastasia        Averi    91.0    67.0   37.1,-43.7\n",
       "77      Hannah        Jerit   142.0    53.0    36.6,10.3\n",
       "78      Jovita   Marquaveon   150.0    70.0   38.2,-32.6\n",
       "79     Jasmine      Jahmal\"   107.0    72.0     38.5,9.1\n",
       "80    Cheyenne       Gerrid    55.0    71.0    36.6,68.2\n",
       "81      Karma\"       Nameer   131.0    65.0    37.5,69.8\n",
       "82    Madeline         Cira   128.0    65.0   37.4,149.6\n",
       "83      Connor     Sheridyn   141.0    73.0   36.8,-80.7\n",
       "84       Sarah       Sunday   180.0    54.0    36.7,-5.6\n",
       "85    Victoria       Maliik   164.0    60.0   36.3,151.2\n",
       "86      George     Tyshonna   105.0    74.0   37.5,-91.5\n",
       "87     Marquis        Kylar   137.0    51.0   38.2,228.4\n",
       "88     Marlene       Fatuma   117.0    64.0    36.4,-9.5\n",
       "89     Kathryn      Diondra   149.0    65.0    38.3,68.0\n",
       "90    Jonathan       Jersey   107.0    77.0    36.0,11.8\n",
       "91      Joshua      Caileen   181.0    67.0    37.3,79.0\n",
       "92       Emily       Leonid   146.0    57.0   37.8,-68.7\n",
       "93   Jacqualyn      Angelea   116.0    63.0   36.4,-31.5\n",
       "94      Gionna       Pamala   180.0    79.0   38.6,-28.6\n",
       "95     Yasmeen       Jahron   135.0    84.0  38.3,-127.3\n",
       "96      Meghan     Carlyann   101.0    66.0    36.6,80.5\n",
       "97        Tess        Shree   146.0    68.0    38.8,64.9\n",
       "98       Maria       Kalvyn   115.0    51.0   37.1,-90.4\n",
       "99       Lexie     Cheyenna   151.0    71.0    35.9,86.1\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = os.path.join('data', 'malformed.csv')\n",
    "df = parse_malformed(fp)\n",
    "cols = ['first', 'last', 'weight', 'height', 'geo']\n",
    "list(df.columns) == cols\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['last'].dtype == np.dtype('O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You're done!\n",
    "\n",
    "* Submit the lab on Gradescope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
